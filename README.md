# SMPR
Системы и методы принятия решения 4 курс 1 семестр

## Содержание

- [Метрические классификаторы](#Метрические-классификаторы)
  - [Алгоритм K ближайших соседей](#Алгоритм-K-ближайших-соседей)
  - [Алгоритм K взвешенных ближайших соседей](#Алгоритм-K-взвешенных-ближайших-соседей)
  - [Метод Парзеновского окна](#Метод-Парзеновского-окна)
  # Метрические классификаторы
   ## Алгоритм K ближайших соседей
   
   KNN(Алгоритм K ближайших соседей) - относит объект к тому классу элементов, которого больше среди k ближайших соседедей.
   
   ### Описание алгоритма

   Все объекты выборки сортируются по расстоянию до объекта, который необходимо классифицировать.
   Код функции сортировки:
   ```r
   sortObjectsByDist <- function(xl, z, metricFunction = euclideanDistance)
   {
    l <- dim(xl)[1]
    n <- dim(xl)[2] - 1
    distances <- matrix(NA, l, 2)
    for (i in 1:l)
    {
      distances[i, ] <- c(i, metricFunction(xl[i, 1:n], z))
    }
    orderedXl <- xl[order(distances[, 2]), ]
    return (orderedXl);
   }
   ```
   В качестве расстояния используется функция:
   ```r
   euclideanDistance <- function(u, v)
   {
    sqrt(sum((u - v)^2))
   }
   ```
   После сортировки подситываем количество объектов каждого класса в первых k объектах отсортированной выборки.
   Относим объект к классу, элементов которого больше среди ближайших соседей. Таким образом код функции knn будет выглядеть следующим образом:

   ```r
   kNN <- function(xl, z, k)
   {
    # Сортируем выборку согласно классифицируемого объекта
    orderedXl <- sortObjectsByDist(xl, z)
    n <- dim(orderedXl)[2] - 1
    # Получаем классы первых k соседей
    classes <- orderedXl[1:k, n + 1]
    # Подсчитываем количество элементов каждого класса
    counts <- table(classes)
    # Находим класс, который доминирует среди первых k соседей
    class <- names(which.max(counts))
    return (class)
   }
   ```

   ### Выбор параметра k с помощью LOO
   Для нахождения оптимального k для алгоритма K ближайших соседей воспользуемся критерием скользящего контроля LOO (Leave One Out).
    
    #### Описание

    Критерий LOO заключается в том, что мы поочередно берем каждый из N элементов выборки и классифицируем его с помощью описанного выше алгоритма на оставшихся N-1 элементах выборки. После классификации проверяем совпал ли класс, который мы определили с его изначальным классом. Таким образом мы можем оценить количество ошибок алгоритма.

    Подсчитав количество ошибок для каждого k от 1 до N, выбираем k, при котором их количество минимально.

    #### Оптимизация
   
    Описанный выше алгоритм использует функцию knn для каждой из точек для каждого k. Таким образом для каждой точки мы сортируем выборку k раз.

    Для того, чтобы не сортировать выборку для одной и той же точки при разном k, оптимизируем алгоритм.
   
    Вместо того, чтобы запускать его для каждого k отдельно. 
    Передадим в функцию вектор из N нулей для записи ошибок.
    Тогда для каждой классифицируемой точки нам необходимо:
    * Отсортировать выборку по расстоянию до этой точки.
    * Для каждого k классифицировать эту точку по отсортированной выборке.

    Чтобы найти оптимальное k нужно взять элемент вектора с минимальным числом ошибок.

    #### Код

    ```r
    looknn <- function(len, data) 
    # len - количество элементов выборки
    # data - выборка
    {
      # Создаем вектор и заполняем его 0
      result = c(rep(0, length( seq(1, len, 1) ) ) )
      # Перебираем элементы выборки
      for(i in c(1:length(data[,1]) ) )
      {
        xl <- data[-i,]
        z <- data[i,1:2]
        # Сортируем выборку
        orderedXl <- sortObjectsByDist(xl, z)
        n <- dim(orderedXl)[2] - 1 
        # Перебираем k от 1 до len
        for (k in seq(1, len, 1))
        {
          # Классифицируем элемент выборки
          classes <- orderedXl[1:k, n+1]
          count <- table(classes)
          class <- names(which.max(count))
          if(class != data[i,3])
          {
            result[k] = result[k] + 1
          }
        }   
      }  
      return(result)
    } 
    ```
 
    #### Результат

    Количество ошибок для каждого k можно увидеть на графике.
 
    ![LOO for KNN](looKNN.PNG)
 
    Таким образом оптимальное k для данного алгоритма равно 6.
 
   ### Карта классификации с помощью алгоритма **KNN**
 
   Построим карту классификации для оптимального k = 6.
   Для этого запустим алгоритм KNN для каждой точки плоскости.
 
   ![Map for KNN](knnPictureNew.PNG)
   
   ## Алгоритм K взвешенных ближайших соседей
   
   ### Описание алгоритма
   Данный алгоритм является модификацией алгоритма K ближайшх соседей. Отличием является то, что при классификации учитывается не только, количество элементов определенного класса, но и расстояние от этих элементов до классифицирумого объекта.
   
   Таким образом, для каждого i-ого соседа вычисляется значение его веса w(i), убывающее с ростом значения i. В данном случае: w(i) = q^i, где q - параметр, принимающий значения от 0 до 1.

   ```r
   #Функция расчета веса w(i)
   weightsKWNN = function(i, q)
   {
      q^i
   }
   ```
   
   ```r
   kwnn <- function(xl, z, k, orderedXl)
   {
    
    n <- dim(orderedXl)[2] - 1
    weights = rep(0,3)
    names(weights) <- c("setosa", "versicolor", "virginica")
    # Сортируем выборку
    classes <- orderedXl[1:k, n+1]
    for(i in 1:k)
    {
      weights[classes[i]] <- weightsKWNN(i,k) + weights[classes[i]];
    }
    class <- names(which.max(weights))
    return (class)
   }

   ```
   ### Выбор параметров k и q с помощью LOO
   Чтобы найти оптимальные значения k и q для алгоритма K взвешенных ближайших соседей воспользуемся критерием скользящего контроля LOO (Leave One Out).

    #### Описание
    С помощью Критерия LOO, описанного [выше](####-Выбор-параметра-k-с-помощью-LOO)
    подсчитаем количество ошибок алгоритма для q от 0 до 1 и для k от 1 до N (N - количество элементов выборки).

    С помощью следующей функции подберем значения k и q, количество ошибок для которых будет минимально.

    ```r
    LOO = function(xl,class) 
    {
      n = dim(xl)[1]
      mininloo = n
      qopt = 0
      kopt = 0
      for(q in seq(0.1, 1, by=0.1))
      {
        loo = rep(0, n-1)
        for(i in 1:(n))
        {
          X=xl[-i, 1:3]
          u=xl[i, 1:2]
          orderedXl <- sortObjectByDist(X, u)
            for(k in 1:(n-1))
            {
              test=kwnn(X,u,k,q,orderedXl)
              if(colors[test] != colors[class[i]])
              {
                loo[k] = loo[k]+1;
              }    
            }
        }
        if(min(loo)<mininloo)
        {
          mininloo = min(loo)
          qopt = q
          kopt = which.min(loo)
        }
      }    
      print(kopt)
      print(qopt)   
     }   
    ```
    Оптимальными значениями являются k = 7, q = 0.6
    Так как невозможно отобразить зависимость количества ошибок от q и k, то покажем на графиках зависимость ошибок от k при оптимальном q и от q при оптимальном k.
    График LOO при оптимальном q | График LOO при оптимальном k
    :---------------------------:|:----------------------------:
    ![](lookwnnk.PNG)|![](lookwnnq.PNG)


   # Метод Парзеновского окна

   ## Описание алгоритма
   PW(Метод Парзеновского окна) - относит объект к тому классу элементов, вес которого больше для данной точки. Вес каждого класса для данной точки определяется функцией ядра, зависящей от расстояния до классифицируемой точки и параметра h - ширины окна.

   Рассмотрим 5 основных типов ядер:
   * Треугольное
    ```r
      kernelT = function(r)
      {  
        return ((1 - abs(r)) * (abs(r) <= 1)) 
      }
    ```
   * Прямоугольное
    ```r
      kernelR = function(r)
      {
        return ((0.5 * (abs(r) <= 1) )) 
      }
    ```
   * Квартическое
    ```r
      kernelQ = function(r)
      {
        return ((15 / 16) * (1 - r ^ 2) ^ 2 * (abs(r) <= 1)) 
      }
    ```
   * Епанечниково 
    ```r
      kernelEP = function(r)
      { 
        return ((3/4*(1-r^2)*(abs(r)<=1))) 
      } 
    ```
   * Гауссовское
    ```r
      kernelG = function(r)
      {
        return (((2*pi)^(-1/2)) * exp(-1/2*r^2)) 
      }
    ```
   ## Реализация алгоритма
   Ниже приведен код функции Парзеновского окна на языке R
   ```r
   pw = function(xl,y,h,metricFunction = euclideanDistance) 
  { 
    n = dim(xl)[1] 
    weights = rep(0,3) 
    names(weights) = c("setosa", "versicolor", "virginica") 
    for(i in 1:n) 
    { 
      x=XL[i,1:2] 
      class=XL[i,3] 
      # Использование параметра h
      r = metricFunction(x,y)/h 
      # Квартическое ядро
      weights[class]=kernelQ(r)+weights[class]; 
      # Треугольное ядро
      #weights[class]=kernelT(r)+weights[class];
      # Прямоугольное ядро
      #weights[class]=kernelR(r)+weights[class];
      # Епанечниково ядро
      #weights[class]=kernelEP(r)+weights[class];
      # Гауссовское ядро
      #weights[class]=kernelG(r)+weights[class];
    } 
    class = names(which.max(weights)) 
    if(max(weights)==0)
    { 
      return ("NA") 
    } 
    else 
    { 
      return (class) 
    } 
  } 
   ``` 
   ## Выбор параметра h с помощью LOO
   Чтобы найти оптимальные значение ширины окна h для Парзеновского окна воспользуемся критерием скользящего контроля LOO (Leave One Out).

    ### Описание
    С помощью Критерия LOO, описанного [выше](####-Выбор-параметра-k-с-помощью-LOO)
    подсчитаем количество ошибок алгоритма для h от 0.1 до 2 при использовнии различных ядер.
    
    Ядро | График
    :---:|:-----------------------------------:
    Треугольное|![](pwlootrian.PNG)
    Прямоугольное|![](loopwrect.PNG)
    Квартическое|![](loopwkvar.PNG)
    Епанечниково|![](loopwep.PNG)
    Гауссовское|![](loopwgauss.PNG)
    
   ## Карты классификации
    
    Ядро | Карта
    :---:|:-----------------------------------:
    Треугольное|![](pwmaptrian.PNG)
    Прямоугольное|![](pwmaprect.PNG)
    Квартическое|![](pwmapkvar.PNG)
    Епанечниково|![](pwmapep.PNG)
    Гауссовское|![](pwmapgauss.PNG)
   




   
